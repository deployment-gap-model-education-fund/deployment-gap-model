"""
Extract gridstatus iso queues data from private bucket archive.

gridstatus code points directly at interconnection queue spreadsheets
on ISO queues websites. These spreadsheets can change without notice
and break the gridstatus API. We have a private archive of the gridstatus data
that allows us to pin the ETL code to a specific version of the raw
data. The version numbers are automatically generated by Google Cloud Storage
Object Versioning.
"""
import logging
import os
from pathlib import Path

import pandas as pd
import pydata_google_auth
from google.cloud import storage

logger = logging.getLogger(__name__)

ISO_QUEUE_VERSIONS: dict[str, str] = {
    "miso": "1681775160487863",
    "caiso": "1681775162586588",
    "pjm": "1681775160979859",
    "ercot": "1681775161342766",
    "spp": "1681775162935809",
    "nyiso": "1681775159356063",
    "isone": "1681775162111351",
}


def extract():
    """Extract gridstatus ISO Queue data."""
    local_cache_dir = Path("/app/data/raw/gridstatus/interconnection_queues/")
    iso_queues: dict[str, pd.DataFrame] = {}
    credentials = None
    for iso, revision_num in ISO_QUEUE_VERSIONS.items():
        filename = f"{iso}.parquet#{revision_num}"
        filepath = local_cache_dir / filename
        if not filepath.exists():
            logger.info(
                f"{filename} not found in {local_cache_dir}. Downloading from GCS bucket."
            )
            bucket_url = "gridstatus-archive"

            GCP_PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
            SCOPES = [
                "https://www.googleapis.com/auth/cloud-platform",
            ]
            if not credentials:
                credentials = pydata_google_auth.get_user_credentials(
                    SCOPES, use_local_webserver=False
                )

            bucket = storage.Client(
                credentials=credentials, project=GCP_PROJECT_ID
            ).bucket(bucket_url, user_project=GCP_PROJECT_ID)
            blob = bucket.blob(
                f"interconnection_queues/{iso}.parquet", generation=revision_num
            )
            with open(filepath, "wb+") as f:
                f.write(blob.download_as_bytes())

        iso_queues[iso] = pd.read_parquet(filepath)

    return iso_queues
